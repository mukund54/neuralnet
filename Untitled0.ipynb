{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELuZ4z_De_Ql",
        "colab_type": "code",
        "outputId": "6f9db462-6983-41c0-afc4-f873e358d888",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#2. Get the file\n",
        "\n",
        "\n",
        "downloaded = drive.CreateFile({'id':'1TjN8pj00ndgSW1iT3gdg8dvVCMRDF8zA'}) # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('newww11.csv')  \n",
        "\n",
        "#3. Read file as panda dataframe\n",
        "import pandas as pd\n",
        "xyz = pd.read_csv('newww11.csv') \n",
        "print(\"hello\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 25.5MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 3.3MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 3.6MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 4.1MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 4.6MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 3.5MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 3.5MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 3.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 450kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 471kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 481kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 501kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 512kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 532kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 542kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 563kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 573kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 593kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 604kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 624kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 634kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 655kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 665kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 686kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 696kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 716kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 727kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 747kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 757kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 901kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 911kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 921kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 931kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 942kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 952kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 962kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 972kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 983kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 3.5MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "hello\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgKTWGhYf3ni",
        "colab_type": "code",
        "outputId": "f556f8e0-06d9-422c-dcb4-b387fce79fdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "import argparse\n",
        "'''\n",
        "url = \"https://drive.google.com/open?id=15BdS8VGc6WXXW6Qyg1NMnaybvQFHQ7wt\"\n",
        "import urllib,os\n",
        "filename = \"hbhi.csv\"\n",
        "if not os.path.isfile(filename):\n",
        "  urllib.request.urlretrieve(url, filename)\n",
        "'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.applications.vgg16 import VGG16\n",
        "#from keras.applications.vgg19 import VGG19\n",
        "\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "from keras.regularizers import l2\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adamax\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.python.saved_model import builder as saved_model_builder\n",
        "from tensorflow.python.saved_model import utils\n",
        "from tensorflow.python.saved_model import tag_constants, signature_constants\n",
        "from tensorflow.python.saved_model.signature_def_utils_impl import build_signature_def, predict_signature_def\n",
        "from tensorflow.contrib.session_bundle import exporter\n",
        "class Namespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "'''\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--csv_file', type=str, default='hbhi.csv')\n",
        "parser.add_argument('--export_path', type=str,default='model_out/')\n",
        "# OPTIONAL\n",
        "parser.add_argument('--batch_size', type=int, default=1)\n",
        "parser.add_argument('--n_epochs', type=int, default=1)\n",
        "parser.add_argument('--debug', dest='debug', action='store_true')\n",
        "'''\n",
        "#FLAGS = parser.parse_args()\n",
        "FLAGS = Namespace(batch_size=1, csv_file='newww11.csv', debug=False, export_path='model_o/', n_epochs=3)\n",
        "\n",
        "if (FLAGS.debug):\n",
        "    FLAGS.batch_size = 10\n",
        "    FLAGS.n_epochs = 1\n",
        "\n",
        "NUM_CLASSES = 2\n",
        "IMG_SIZE = 48\n",
        "\n",
        "# TODO: Use the 'Usage' field to separate based on training/testing\n",
        "TRAIN_END = 2517\n",
        "TEST_START = TRAIN_END + 1\n",
        "\n",
        "\n",
        "def split_for_test(list):\n",
        "    train = list[0:TRAIN_END]\n",
        "    test = list[TEST_START:]\n",
        "    return train, test\n",
        "\n",
        "\n",
        "def pandas_vector_to_list(pandas_df):\n",
        "    py_list = [item[0] for item in pandas_df.values.tolist()]\n",
        "    return py_list\n",
        "\n",
        "\n",
        "def process_emotion(emotion):\n",
        "    \"\"\"\n",
        "    Takes in a vector of emotions and outputs a list of emotions as one-hot vectors.\n",
        "    :param emotion: vector of ints (0-7)\n",
        "    :return: list of one-hot vectors (array of 7)\n",
        "    \"\"\"\n",
        "    emotion_as_list = pandas_vector_to_list(emotion)\n",
        "    y_data = []\n",
        "    for index in range(len(emotion_as_list)):\n",
        "        y_data.append(emotion_as_list[index])\n",
        "\n",
        "    # Y data\n",
        "    y_data_categorical = np_utils.to_categorical(y_data, NUM_CLASSES)\n",
        "    return y_data_categorical\n",
        "\n",
        "\n",
        "def process_pixels(pixels, img_size=IMG_SIZE):\n",
        "    \"\"\"\n",
        "    Takes in a string (pixels) that has space separated ints. Will transform the ints\n",
        "    to a 48x48 matrix of floats(/255).\n",
        "    :param pixels: string with space separated ints\n",
        "    :param img_size: image size\n",
        "    :return: array of 48x48 matrices\n",
        "    \"\"\"\n",
        "    pixels_as_list = pandas_vector_to_list(pixels)\n",
        "\n",
        "    np_image_array = []\n",
        "    for index, item in enumerate(pixels_as_list):\n",
        "        # 48x48\n",
        "        data = np.zeros((img_size, img_size,3), dtype=np.uint8)\n",
        "        # split space separated ints\n",
        "        pixel_data = item.split()\n",
        "        \n",
        "        arr = np.reshape(pixel_data,(48,48,3))\n",
        "        '''print(arr.shape)\n",
        "        exit()\n",
        "        #print(pixel_data)\n",
        "        print('img_size',img_size)\n",
        "        \n",
        "        # 0 -> 47, loop through the rows\n",
        "        for i in range(0, img_size):\n",
        "            # (0 = 0), (1 = 47), (2 = 94), ...\n",
        "            pixel_index = i * img_size\n",
        "            print(pixel_index)\n",
        "            # (0 = [0:47]), (1 = [47: 94]), (2 = [94, 141]), ...\n",
        "            data[i] = pixel_data[pixel_index:pixel_index + img_size]\n",
        "            print(data[i])\n",
        "        exit()\n",
        "        '''\n",
        "\n",
        "        np_image_array.append(arr)\n",
        "\n",
        "    np_image_array = np.array(np_image_array)\n",
        "    # convert to float and divide by 255\n",
        "    np_image_array = np_image_array.astype('float32') / 255.0\n",
        "    return np_image_array\n",
        "\n",
        "\n",
        "def get_vgg16_output(vgg16, array_input, n_feature_maps):\n",
        "    vg_input = array_input\n",
        "\n",
        "    picture_train_features = vgg16.predict(vg_input)\n",
        "    del (vg_input)\n",
        "\n",
        "    feature_map = np.empty([n_feature_maps, 512])\n",
        "    for idx_pic, picture in enumerate(picture_train_features):\n",
        "        feature_map[idx_pic] = picture\n",
        "    return feature_map\n",
        "\n",
        "\n",
        "def duplicate_input_layer(array_input, size):\n",
        "    vg_input = np.empty([size, 48, 48, 3])\n",
        "    for index, item in enumerate(vg_input):\n",
        "        item[:, :, 0] = array_input[index]\n",
        "        item[:, :, 1] = array_input[index]\n",
        "        item[:, :, 2] = array_input[index]\n",
        "    return vg_input\n",
        "\n",
        "\n",
        "def main():\n",
        "    # used to get the session/graph data from keras\n",
        "    K.set_learning_phase(0)\n",
        "        \n",
        "    # get the data in a Pandas dataframe\n",
        "    raw_data = pd.read_csv(FLAGS.csv_file)\n",
        "\n",
        "    # convert to one hot vectors\n",
        "    emotion_array = process_emotion(raw_data[['Emotion']])\n",
        "    # convert to a 48x48 float matrix\n",
        "    pixel_array = process_pixels(raw_data[['Pixel']])\n",
        "\n",
        "    # split for test/train\n",
        "    y_train, y_test = split_for_test(emotion_array)\n",
        "    x_train_matrix, x_test_matrix = split_for_test(pixel_array)\n",
        "\n",
        "    n_train = int(len(x_train_matrix))\n",
        "    n_test = int(len(x_test_matrix))\n",
        "\n",
        "    x_train_input = x_train_matrix\n",
        "    x_test_input = x_test_matrix\n",
        "\n",
        "    # vgg 16. include_top=False so the output is the 512 and use the learned weights\n",
        "    vgg16 = VGG16(include_top=False, input_shape=(48, 48, 3), pooling='avg', weights='imagenet')\n",
        "\n",
        "    # get vgg16 outputs\n",
        "    x_train_feature_map = get_vgg16_output(vgg16, x_train_matrix, n_train)\n",
        "    x_test_feature_map = get_vgg16_output(vgg16, x_test_matrix, n_test)\n",
        "\n",
        "    # build and train model\n",
        "    top_layer_model = Sequential()\n",
        "    top_layer_model.add(Dense(256, input_shape=(512,), activation='relu'))\n",
        "    top_layer_model.add(Dropout(128/256))\n",
        "    top_layer_model.add(Dense(128, input_shape=(256,), activation='relu'))\n",
        "    top_layer_model.add(Dropout(40/128))\n",
        "    top_layer_model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        "\n",
        "    adamax = Adamax()\n",
        "\n",
        "    top_layer_model.compile(loss='categorical_crossentropy',\n",
        "                            optimizer=adamax, metrics=['accuracy'])\n",
        "\n",
        "    # train\n",
        "    top_layer_model.fit(x_train_feature_map, y_train,\n",
        "                        validation_data=(x_train_feature_map, y_train),\n",
        "                        nb_epoch=FLAGS.n_epochs, batch_size=FLAGS.batch_size)\n",
        "    # Evaluate\n",
        "    score = top_layer_model.evaluate(x_test_feature_map,\n",
        "                                     y_test, batch_size=FLAGS.batch_size)\n",
        "\n",
        "    print(\"After top_layer_model training (test set): {}\".format(score))\n",
        "\n",
        "    # Merge two models and create the final_model_final_final\n",
        "    inputs = Input(shape=(48, 48, 3))\n",
        "    vg_output = vgg16(inputs)\n",
        "    print(\"vg_output: {}\".format(vg_output.shape))\n",
        "    # TODO: the 'pooling' argument of the VGG16 model is important for this to work otherwise you will have to  squash\n",
        "    # output from (?, 1, 1, 512) to (?, 512)\n",
        "    model_predictions = top_layer_model(vg_output)\n",
        "    final_model = Model(input=inputs, output=model_predictions)\n",
        "    final_model.compile(loss='categorical_crossentropy',\n",
        "                        optimizer=adamax, metrics=['accuracy'])\n",
        "    final_model_score = final_model.evaluate(x_train_input,\n",
        "                                             y_train, batch_size=FLAGS.batch_size)\n",
        "    print(final_model.metrics_names)\n",
        "\n",
        "    print(\"Sanity check - final_model (train score): {}\".format(final_model_score))\n",
        "\n",
        "    final_model_score = final_model.evaluate(x_test_input,\n",
        "                                             y_test, batch_size=FLAGS.batch_size)\n",
        "    print(\"Sanity check - final_model (test score): {}\".format(final_model_score))\n",
        "    # config = final_model.get_config()\n",
        "    # weights = final_model.get_weights()\n",
        "\n",
        "    # probably don't need to create a new model\n",
        "    # model_to_save = Model.from_config(config)\n",
        "    # model_to_save.set_weights(weights)\n",
        "    model_to_save = final_model\n",
        "    model_to_save.save('path_to_my_model.h5')\n",
        "    #print(\"F1 score for {} regularization is\".format(leo))\n",
        "    #print(f1)\n",
        "    print(\"Model input name: {}\".format(model_to_save.input))\n",
        "    print(\"Model output name: {}\".format(model_to_save.output))\n",
        "        \n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:189: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 2517 samples, validate on 2517 samples\n",
            "Epoch 1/3\n",
            "2517/2517 [==============================] - 10s 4ms/step - loss: 0.5157 - acc: 0.7330 - val_loss: 0.4782 - val_acc: 0.7473\n",
            "Epoch 2/3\n",
            "2517/2517 [==============================] - 9s 4ms/step - loss: 0.4020 - acc: 0.8077 - val_loss: 0.3285 - val_acc: 0.8554\n",
            "Epoch 3/3\n",
            "2517/2517 [==============================] - 11s 4ms/step - loss: 0.3248 - acc: 0.8482 - val_loss: 0.2468 - val_acc: 0.8983\n",
            "38/38 [==============================] - 0s 1ms/step\n",
            "After top_layer_model training (test set): [0.7465764297632248, 0.7894736842105263]\n",
            "vg_output: (?, 512)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:203: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"se...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2517/2517 [==============================] - 10s 4ms/step\n",
            "['loss', 'acc']\n",
            "Sanity check - final_model (train score): [0.24683931833306605, 0.8982916170043703]\n",
            "38/38 [==============================] - 0s 4ms/step\n",
            "Sanity check - final_model (test score): [0.7465777034938966, 0.7894736842105263]\n",
            "Model input name: Tensor(\"input_12:0\", shape=(?, 48, 48, 3), dtype=float32)\n",
            "Model output name: Tensor(\"sequential_6/dense_17/Softmax:0\", shape=(?, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_VQAISYf8WS",
        "colab_type": "code",
        "outputId": "5aeae307-de05-4c60-8e03-ee45a538f20b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1183
        }
      },
      "source": [
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "from keras.models import Model,load_model\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adamax\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.python.saved_model import builder as saved_model_builder\n",
        "from tensorflow.python.saved_model import utils\n",
        "from tensorflow.python.saved_model import tag_constants, signature_constants\n",
        "from tensorflow.python.saved_model.signature_def_utils_impl import build_signature_def, predict_signature_def\n",
        "from tensorflow.contrib.session_bundle import exporter\n",
        "class Namespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "'''\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--csv_file', type=str, default='hbhi.csv')\n",
        "parser.add_argument('--export_path', type=str,default='model_out/')\n",
        "# OPTIONAL\n",
        "parser.add_argument('--batch_size', type=int, default=1)\n",
        "parser.add_argument('--n_epochs', type=int, default=1)\n",
        "parser.add_argument('--debug', dest='debug', action='store_true')\n",
        "'''\n",
        "#FLAGS = parser.parse_args()\n",
        "FLAGS = Namespace(batch_size=1, csv_file='newww11.csv', debug=False, export_path='model_out/', n_epochs=1)\n",
        "\n",
        "if (FLAGS.debug):\n",
        "    FLAGS.batch_size = 10\n",
        "    FLAGS.n_epochs = 1\n",
        "\n",
        "NUM_CLASSES = 2\n",
        "IMG_SIZE = 48\n",
        "\n",
        "# TODO: Use the 'Usage' field to separate based on training/testing\n",
        "TRAIN_END = 2517\n",
        "TEST_START = TRAIN_END + 1\n",
        "raw_data = pd.read_csv(FLAGS.csv_file)\n",
        "\n",
        "# convert to one hot vectors\n",
        "emotion_array = process_emotion(raw_data[['Emotion']])\n",
        "# convert to a 48x48 float matrix\n",
        "pixel_array = process_pixels(raw_data[['Pixel']])\n",
        "\n",
        "# split for test/train\n",
        "y_train, y_test = split_for_test(emotion_array)\n",
        "x_train_matrix, x_test_matrix = split_for_test(pixel_array)\n",
        "print(x_test_matrix.dtype)\n",
        "\n",
        "n_train = int(len(x_train_matrix))\n",
        "n_test = int(len(x_test_matrix))\n",
        "\n",
        "x_train_input = x_train_matrix\n",
        "x_test_input = x_test_matrix\n",
        "\n",
        "with tf.Session(graph=tf.Graph()) as sess:\n",
        "      '''\n",
        "      tf.saved_model.loader.load(sess, [\"serve\"], FLAGS.export_path)\n",
        "      graph = tf.get_default_graph()\n",
        "      #for i in graph.get_operations():\n",
        "       # print(i)\n",
        "      images = graph.get_tensor_by_name(\"duplicate_input_layer:0\")\n",
        "      #scores = graph.get_tensor_by_name(\"scores\")\n",
        "      '''\n",
        "      new_model = load_model('path_to_my_model.h5')\n",
        "      new_model.summary()\n",
        "      x=x_test_input\n",
        "      \n",
        "      y=new_model.predict(x)\n",
        "      true_positive=0\n",
        "      false_positive=0\n",
        "      false_negative=0\n",
        "      true_negative=0\n",
        "      for (i,j) in zip(y,y_test):\n",
        "        print(i,\"    \",j)\n",
        "      for (i,j) in zip(np.round(y),y_test):  \n",
        "        if(i[0]==j[0]==1. and i[1]==j[1]==0.):\n",
        "          true_negative+=1\n",
        "        if(i[0]==j[0]==0. and i[1]==j[1]==1.):\n",
        "          true_positive+=1\n",
        "        if(i[0]==j[1]==1. and i[1]==j[0]==0.):\n",
        "          false_negative+=1\n",
        "        if(i[0]==j[1]==0. and i[1]==j[0]==1.):\n",
        "          false_positive+=1\n",
        "      print(\"presision\")\n",
        "      pre=true_positive/(true_positive+false_positive)\n",
        "      print(pre)\n",
        "      print(\"recall\")\n",
        "      recal=true_positive/(true_positive+false_negative)\n",
        "      print(recal)\n",
        "      print(\"f1-score\")\n",
        "      f1=2*(pre*recal)/(pre+recal)\n",
        "      print(f1)\n",
        "      print(\"accuracy\")\n",
        "      acu=(true_positive+true_negative)/y.shape[0]\n",
        "      print(acu)\n",
        "      print(y.shape[0])\n",
        "      sess.close()\n",
        "      \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "float32\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:327: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "  warnings.warn('Error in loading the saved optimizer '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_12 (InputLayer)        (None, 48, 48, 3)         0         \n",
            "_________________________________________________________________\n",
            "vgg16 (Model)                (None, 512)               14714688  \n",
            "_________________________________________________________________\n",
            "sequential_6 (Sequential)    (None, 2)                 164482    \n",
            "=================================================================\n",
            "Total params: 14,879,170\n",
            "Trainable params: 14,879,170\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[0.00964922 0.9903508 ]      [0. 1.]\n",
            "[0.08955894 0.91044104]      [0. 1.]\n",
            "[0.07765368 0.92234635]      [1. 0.]\n",
            "[0.9963146  0.00368548]      [1. 0.]\n",
            "[0.04842232 0.95157766]      [0. 1.]\n",
            "[0.651751   0.34824902]      [1. 0.]\n",
            "[0.0236025 0.9763975]      [0. 1.]\n",
            "[0.9636306  0.03636939]      [0. 1.]\n",
            "[0.53580815 0.46419182]      [1. 0.]\n",
            "[0.97654235 0.02345769]      [1. 0.]\n",
            "[0.09303849 0.90696156]      [0. 1.]\n",
            "[0.5899468  0.41005316]      [1. 0.]\n",
            "[0.06824398 0.931756  ]      [1. 0.]\n",
            "[0.7961085  0.20389155]      [1. 0.]\n",
            "[0.14554834 0.85445166]      [0. 1.]\n",
            "[0.95203525 0.04796473]      [0. 1.]\n",
            "[0.06279476 0.9372052 ]      [0. 1.]\n",
            "[0.6664167  0.33358327]      [1. 0.]\n",
            "[0.8275778  0.17242216]      [1. 0.]\n",
            "[0.9535695  0.04643051]      [1. 0.]\n",
            "[0.99469876 0.00530124]      [1. 0.]\n",
            "[9.9997532e-01 2.4669724e-05]      [1. 0.]\n",
            "[0.88349134 0.11650869]      [0. 1.]\n",
            "[0.47231224 0.5276878 ]      [0. 1.]\n",
            "[0.01506547 0.98493457]      [1. 0.]\n",
            "[0.9785905  0.02140944]      [1. 0.]\n",
            "[0.2610574 0.7389426]      [0. 1.]\n",
            "[0.00996796 0.9900321 ]      [0. 1.]\n",
            "[0.28052765 0.71947235]      [0. 1.]\n",
            "[0.15710576 0.8428942 ]      [0. 1.]\n",
            "[0.6554374 0.3445626]      [1. 0.]\n",
            "[0.48105356 0.51894647]      [0. 1.]\n",
            "[0.9477442 0.0522558]      [0. 1.]\n",
            "[0.9813579  0.01864206]      [1. 0.]\n",
            "[0.92834765 0.07165229]      [1. 0.]\n",
            "[0.02754783 0.9724522 ]      [0. 1.]\n",
            "[0.2248818 0.7751182]      [1. 0.]\n",
            "[0.714387   0.28561297]      [1. 0.]\n",
            "presision\n",
            "0.7777777777777778\n",
            "recall\n",
            "0.7777777777777778\n",
            "f1-score\n",
            "0.7777777777777778\n",
            "accuracy\n",
            "0.7894736842105263\n",
            "38\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}